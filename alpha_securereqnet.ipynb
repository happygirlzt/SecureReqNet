{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/danaderp/SecureReqNet/blob/master/alpha_securereqnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeZw_wJ0fvvF"
   },
   "source": [
    "## Alpha-SecureReqNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zweGRad_d4va"
   },
   "outputs": [],
   "source": [
    "#danaderp May6'19\n",
    "#Prediction For Main Issues Data Set \n",
    "#alpha-SecureReqNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tV4BxbkCfu09"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wMymf9NRaYNc",
    "outputId": "7daddf17-fc70-498d-d268-99f7a16d67b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/happygirlzt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "URHlMuyvgZsW",
    "outputId": "7161654a-f9e1-4ed0-e6a3-2e1d7e18a190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "#Importing Neural Dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dot, Input, Dense, Reshape, LSTM, Conv2D, Flatten, MaxPooling1D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import Embedding, Multiply, Subtract\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMqMwL02giiq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == 'code':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPfRbyEiGdx-"
   },
   "outputs": [],
   "source": [
    "from data.read_data import Dynamic_Dataset\n",
    "from data.read_data import Processing_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sPpIvKEJQ5v"
   },
   "source": [
    "### Loading word embeddings from previous compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_risxO7JRmy"
   },
   "outputs": [],
   "source": [
    "path = \"data/augmented_dataset/\" #Place here the dataset you want to process\n",
    "process_unit = Processing_Dataset(path)\n",
    "ground_truth = process_unit.get_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0XUh1VSTJqX2",
    "outputId": "ad2b28fc-7f93-4b75-9fd5-84ddb07cfbcf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11612 104510\n",
      "('(1,0)', 'Vulnerability in the Java SE, Java SE Embedded component of Oracle Java SE (subcomponent: Libraries). The supported version that is affected is Java SE: 8u131; Java SE Embedded: 8u131. Easily exploitable vulnerability allows unauthenticated attacker with network access via multiple protocols to compromise Java SE, Java SE Embedded. Successful attacks require human interaction from a person other than the attacker and while the vulnerability is in Java SE, Java SE Embedded, attacks may significantly impact additional products. Successful attacks of this vulnerability can result in takeover of Java SE, Java SE Embedded. Note: This vulnerability applies to Java deployments, typically in clients running sandboxed Java Web Start applications or sandboxed Java applets, that load and run untrusted code (e.g., code that comes from the internet) and rely on the Java sandbox for security. This vulnerability does not apply to Java deployments, typically in servers, that load and run only trusted code (e.g., code installed by an administrator). CVSS 3.0 Base Score 9.6 (Confidentiality, Integrity and Availability impacts). CVSS Vector: (CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H).') ('(1,0)', 'The currently used Rails version, in the stable branch, is insecure\\n\\nYou should update the Gemfile.lock to hotfix this.\\n\\nhttp://weblog.rubyonrails.org/2014/2/18/Rails_3_2_17_4_0_3_and_4_1_0_beta2_have_been_released/')\n"
     ]
    }
   ],
   "source": [
    "dataset = Dynamic_Dataset(ground_truth, path)\n",
    "test, train = process_unit.get_test_and_training(ground_truth)\n",
    "print(len(test),len(train))\n",
    "print(test[0],train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbNp-1h0KATi"
   },
   "outputs": [],
   "source": [
    "#Preprocesing Corpora\n",
    "# embeddings = Embeddings\n",
    "max_words = 5000 #<------- [Parameter]\n",
    "pre_corpora_train = [doc for doc in train if len(doc[1])< max_words]\n",
    "pre_corpora_test = [doc for doc in test if len(doc[1])< max_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103872\n",
      "11543\n"
     ]
    }
   ],
   "source": [
    "print(len(pre_corpora_train))\n",
    "print(len(pre_corpora_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17227\n",
      "Index(['use', 'user', 'version', 'test', 'file', 'secur', 'vulner', 'issu',\n",
      "       'allow', 'new',\n",
      "       ...\n",
      "       'enfield', 'hvidovr', 'madura', 'nonotak', 'bazeley', 'kunnam',\n",
      "       'steelesvill', 'pessoa', 'morisada', 'kuttin'],\n",
      "      dtype='object', length=17227)\n"
     ]
    }
   ],
   "source": [
    "# Rewrite cell above\n",
    "import pandas as pd\n",
    "embed_path = 'data/word_embeddings-embed_size_100-epochs_100.csv'\n",
    "df = pd.read_csv(embed_path)\n",
    "cols = df.columns[1:]\n",
    "\n",
    "print(len(cols))\n",
    "print(cols)\n",
    "df.reset_index()\n",
    "embeddings_dict = {}\n",
    "for i in range(len(cols)):\n",
    "    embeddings_dict[cols[i]] = list(df.iloc[:,i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '1', ..., '0', '1', '0'], dtype='<U1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_train_mod = []\n",
    "\n",
    "y_train=[]\n",
    "for row in pre_corpora_train:\n",
    "    corpora_train_mod.append(row[1])\n",
    "    y_train.append(row[0][1])\n",
    "    \n",
    "np.asarray(y_train)\n",
    "corpora_test_mod=[]\n",
    "y_test=[]\n",
    "for row in pre_corpora_test:\n",
    "    corpora_test_mod.append(row[1])\n",
    "    y_test.append(row[0][1])\n",
    "    \n",
    "np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train data is 103872\n",
      "the size of train data is 103872\n",
      "the size of test data is 11543\n",
      "the size of test data is 11543\n"
     ]
    }
   ],
   "source": [
    "print('the size of train data is {}'.format(len(corpora_train_mod)))\n",
    "print('the size of train data is {}'.format(len(y_train)))\n",
    "print('the size of test data is {}'.format(len(corpora_test_mod)))\n",
    "print('the size of test data is {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the train, test data to csv\n",
    "pd.DataFrame(corpora_train_mod).to_csv('train_raw_sentences.csv', index=False)\n",
    "pd.DataFrame(corpora_test_mod).to_csv('test_raw_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the train, test labels to csv\n",
    "pd.DataFrame(y_train).to_csv('train_labels.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=pd.read_csv('train_raw_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103872, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The currently used Rails version, in the stabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'This is a useful security improvement, that I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We just had an admin accidentally push to a br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'It's possible to check publicly if a private ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I agree that people are using self-signed cert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103867</th>\n",
       "      <td>Cross-site scripting (XSS) vulnerability in in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103868</th>\n",
       "      <td>SQL injection vulnerability in dispatch.php in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103869</th>\n",
       "      <td>Cisco ASR 1000 devices with software before 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103870</th>\n",
       "      <td>Multiple cross-site scripting (XSS) vulnerabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103871</th>\n",
       "      <td>Multiple SQL injection vulnerabilities in dotP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103872 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0       The currently used Rails version, in the stabl...\n",
       "1       'This is a useful security improvement, that I...\n",
       "2       We just had an admin accidentally push to a br...\n",
       "3       'It's possible to check publicly if a private ...\n",
       "4       I agree that people are using self-signed cert...\n",
       "...                                                   ...\n",
       "103867  Cross-site scripting (XSS) vulnerability in in...\n",
       "103868  SQL injection vulnerability in dispatch.php in...\n",
       "103869  Cisco ASR 1000 devices with software before 3....\n",
       "103870  Multiple cross-site scripting (XSS) vulnerabil...\n",
       "103871  Multiple SQL injection vulnerabilities in dotP...\n",
       "\n",
       "[103872 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=100\n",
    "\n",
    "def nomalizer(sent):\n",
    "    only_letters=re.sub(\"[^a-zA-Z]\", \" \", sent)\n",
    "    tokens=nltk.word_tokenize(only_letters)[2:]\n",
    "    lower_case=[l.lower() for l in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer=WordNetLemmatizer()\n",
    "    filtered_result=list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas=[wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas\n",
    "\n",
    "# Generate a sentence vector by averaging words embeddings in a sentence\n",
    "def generate_sentence_vec(sentence, embeddings_dict, num_features):\n",
    "    sentence_vec=np.zeros(num_features,dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    \n",
    "    embeddings_set=set(embeddings_dict.keys())\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in embeddings_set:\n",
    "            num_words+=1\n",
    "            sentence_vec=np.add(sentence_vec, embeddings_dict[word])\n",
    "    \n",
    "    sentence_vec=np.divide(sentence_vec,num_words)\n",
    "    #print(sentence_vec)\n",
    "    return sentence_vec\n",
    "\n",
    "# Generate all senteneces vectors\n",
    "def generate_average_sentences_vec(sentences,embeddings_dict,num_features):\n",
    "    count=0\n",
    "    total_sent=len(sentences)\n",
    "    sentences_vec=np.zeros((len(sentences),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Sentences {} of {}\".format(count,total_sent))\n",
    "\n",
    "        sentences_vec[count] = generate_sentence_vec(sentence, embeddings_dict, num_features)\n",
    "        count+=1\n",
    "\n",
    "    return sentences_vec\n",
    "\n",
    "def get_sentence_vectors(raw_data):\n",
    "    data = []\n",
    "    print(len(raw_data))\n",
    "    print(\"---- Nomalizing ----\")\n",
    "    for sent in raw_data:\n",
    "        data.append(nomalizer(sent))\n",
    "    \n",
    "    print(len(data))\n",
    "    X=generate_average_sentences_vec(data, embeddings_dict, num_features)\n",
    "    return X\n",
    "\n",
    "X_train_data = get_sentence_vectors(corpora_train_mod)\n",
    "X_test_data=get_sentence_vectors(corpora_test_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_train = X_train_data\n",
    "corpora_test = X_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,6))\n",
    "sns.countplot(y_train)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.show()\n",
    "#fig.savefig('original_distribution.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_train_data=SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(X_train_data)\n",
    "X_test_data=SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf=np.zeros(shape=(len(y_test),1))\n",
    "y_pred_svc=np.zeros(shape=(len(y_test),1))\n",
    "y_pred_nb=np.zeros(shape=(len(y_test),1))\n",
    "\n",
    "def train_clf(name, clf, y_pred, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    acc=accuracy_score(y_test, y_pred) \n",
    "\n",
    "    y_pred_proba = clf.predict_proba(x_test)\n",
    "    auc=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "\n",
    "    \n",
    "    print(\"{} accuracy is {}\".format(name, acc))\n",
    "    print(\"{} AUC is {}\".format(name, auc))\n",
    "    \n",
    "train_clf('Random_Forest', RandomForestClassifier(), y_pred_rf,X_train_data,y_train,X_test_data,y_test)\n",
    "train_clf('SVC',SVC(gamma='auto',probability=True), y_pred_svc,X_train_data,y_train,X_test_data,y_test)\n",
    "train_clf('Naive bayes',GaussianNB(), y_pred_nb,X_train_data,y_train,X_test_data,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = [[int(list(doc[0])[1]),int(list(doc[0])[3])] for doc in pre_corpora_train]#vectorization Output\n",
    "target_test = [[int(list(doc[0])[1]),int(list(doc[0])[3])]for doc in pre_corpora_test]#vectorization Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sentences_train = max([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "max_len_sentences_test = max([len(doc) for doc in corpora_test]) #<------- [Parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "84Y6sWgPKgkj",
    "outputId": "faf14c6d-926f-4256-b737-ce2541e30b73"
   },
   "outputs": [],
   "source": [
    "max_len_sentences = max(max_len_sentences_train,max_len_sentences_test)\n",
    "print(\"Max. Sentence # words:\",max_len_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_sentences_train = min([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "min_len_sentences_test = min([len(doc) for doc in corpora_test]) #<------- [Parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DG6fdraDKjau",
    "outputId": "8f2ff01c-1374-465a-ca7c-c847320b1576"
   },
   "outputs": [],
   "source": [
    "min_len_sentences = max(min_len_sentences_train,min_len_sentences_test)\n",
    "print(\"Min. Sentence # words:\",min_len_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osMqB7LMKlCO"
   },
   "outputs": [],
   "source": [
    "embed_size = np.size(corpora_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKY-ieldfnVQ"
   },
   "source": [
    "### Designing Baseline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfQ2znRCK0MS"
   },
   "outputs": [],
   "source": [
    "#BaseLine Architecture <-------\n",
    "embeddigs_cols = embed_size\n",
    "input_sh = (max_len_sentences,embeddigs_cols,1)\n",
    "#Selecting filters? \n",
    "#https://stackoverflow.com/questions/48243360/how-to-determine-the-filter-parameter-in-the-keras-conv2d-function\n",
    "#https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional\n",
    "\n",
    "N_filters = 128 # <-------- [HyperParameter] Powers of 2 Numer of Features\n",
    "K = 2 # <-------- [HyperParameter] Number of Classess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "glpiGw14K-ch",
    "outputId": "32e23066-c2fd-451d-9656-6975beb6c229"
   },
   "outputs": [],
   "source": [
    "gram_input = Input(shape = input_sh)\n",
    "# 1st Convolutional Layer Convolutional Layer (7-gram)\n",
    "conv_1_layer = Conv2D(filters=32, input_shape=input_sh, activation='relu', \n",
    "                      kernel_size=(7,embeddigs_cols), padding='valid')(gram_input)\n",
    "conv_1_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CDLlKGWFLBQR",
    "outputId": "21426f83-311e-40d2-e163-1de09a6c1f37"
   },
   "outputs": [],
   "source": [
    "# Max Pooling \n",
    "max_1_pooling = MaxPooling2D(pool_size=((max_len_sentences-7+1),1), strides=None, padding='valid')(conv_1_layer)\n",
    "max_1_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NIPAYP28LDXv",
    "outputId": "1e24617f-9be5-41e9-faf7-11fc4f581204"
   },
   "outputs": [],
   "source": [
    "# Fully Connected layer\n",
    "fully_connected_1_gram = Flatten()(max_1_pooling)\n",
    "fully_connected_1_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YL4Rk9RyLFBF",
    "outputId": "c80a8898-d8f7-436c-849a-401f0e737a05"
   },
   "outputs": [],
   "source": [
    "fully_connected_1_gram = Reshape((32, 1, 1))(fully_connected_1_gram)\n",
    "fully_connected_1_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "13c97VKNLIDs",
    "outputId": "daa22b71-3994-4d3d-e2aa-31dac4d3a07f"
   },
   "outputs": [],
   "source": [
    "# 2nd Convolutional Layer (5-gram)\n",
    "conv_2_layer = Conv2D(filters=64, kernel_size=(5,1), activation='relu', \n",
    "                      padding='valid')(fully_connected_1_gram)\n",
    "conv_2_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FoIm4mTZLKsx",
    "outputId": "7b1310c0-5e2d-4ceb-f9e2-3944c6162bbf"
   },
   "outputs": [],
   "source": [
    "max_2_pooling = MaxPooling2D(pool_size=((32-5+1),1), strides=None, padding='valid')(conv_2_layer)\n",
    "max_2_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3BUO38gRLMoy",
    "outputId": "9d497c9c-1e1f-491c-d2f1-0ea6030b9462"
   },
   "outputs": [],
   "source": [
    "# Fully Connected layer\n",
    "fully_connected_2_gram = Flatten()(max_2_pooling)\n",
    "fully_connected_2_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m3t8vnrrLPDi",
    "outputId": "a9464f2a-ae71-4dcf-b2ab-306fc8cbaec8"
   },
   "outputs": [],
   "source": [
    "fully_connected_2_gram = Reshape((64, 1, 1))(fully_connected_2_gram)\n",
    "fully_connected_2_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TRBrLT6hLQ4x",
    "outputId": "aa9efea5-1983-4bc4-b6f5-2bd84e25b5f1"
   },
   "outputs": [],
   "source": [
    "# 3rd Convolutional Layer (3-gram)\n",
    "conv_3_layer =  Conv2D(filters=128, kernel_size=(3,1), activation='relu', \n",
    "                      padding='valid')(fully_connected_2_gram)\n",
    "conv_3_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3fqEy0OvLTo3",
    "outputId": "6599672b-dd13-47ed-b0b9-e3737456664a"
   },
   "outputs": [],
   "source": [
    "# 4th Convolutional Layer (3-gram)\n",
    "conv_4_layer = Conv2D(filters=128, kernel_size=(3,1), activation='relu', \n",
    "                     padding='valid')(conv_3_layer)\n",
    "conv_4_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wVv4RFv6LYYt",
    "outputId": "30eb2c76-c0e0-4153-8428-73f3493d5074"
   },
   "outputs": [],
   "source": [
    "# 5th Convolutional Layer (3-gram)\n",
    "conv_5_layer = Conv2D(filters=64, kernel_size=(3,1), activation='relu', \n",
    "                     padding='valid')(conv_4_layer)\n",
    "conv_5_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l4hqFvdHLaRZ",
    "outputId": "579163a8-4e80-42b4-bfad-0d467ac3eb31"
   },
   "outputs": [],
   "source": [
    "# Max Pooling\n",
    "max_5_pooling = MaxPooling2D(pool_size=(58,1), strides=None, padding='valid')(conv_5_layer)\n",
    "max_5_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tbgwf1kgLdzW",
    "outputId": "3b76a2b2-88a1-42db-d726-4c3c09de755a"
   },
   "outputs": [],
   "source": [
    "# Fully Connected layer\n",
    "fully_connected = Flatten()(max_5_pooling)\n",
    "fully_connected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bc0USw7yLiBk",
    "outputId": "f5cbed89-d7a1-4938-c57f-0d40e8971b96"
   },
   "outputs": [],
   "source": [
    "# 1st Fully Connected Layer\n",
    "deep_dense_1_layer = Dense(32, activation='relu')(fully_connected)\n",
    "deep_dense_1_layer = Dropout(0.2)(deep_dense_1_layer) # <-------- [HyperParameter]\n",
    "deep_dense_1_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cPCwpofQLj5j",
    "outputId": "61eda5c0-c42b-4a99-d87a-ffc121e02f00"
   },
   "outputs": [],
   "source": [
    "# 2nd Fully Connected Layer\n",
    "deep_dense_2_layer = Dense(32, activation='relu')(deep_dense_1_layer)\n",
    "deep_dense_2_layer = Dropout(0.2)(deep_dense_2_layer) # <-------- [HyperParameter]\n",
    "deep_dense_2_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4PK2LvkbLrFJ",
    "outputId": "f23d53c1-b4cb-4b1f-9aec-1c4cb69c526d"
   },
   "outputs": [],
   "source": [
    "# 3rd Fully Connected Layer\n",
    "deep_dense_3_layer = Dense(16, activation='relu')(deep_dense_2_layer)\n",
    "deep_dense_3_layer = Dropout(0.2)(deep_dense_3_layer) # <-------- [HyperParameter]\n",
    "deep_dense_3_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjjeoJYGLwFp"
   },
   "outputs": [],
   "source": [
    "predictions = Dense(K, activation='softmax')(deep_dense_3_layer)\n",
    "#Criticality Model\n",
    "criticality_network = Model(inputs=[gram_input],outputs=[predictions]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "wmoUPuPqL2z3",
    "outputId": "7c749f96-777c-4cfb-afe0-b364195506d1"
   },
   "outputs": [],
   "source": [
    "print(criticality_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1v_kHaUL6mt"
   },
   "outputs": [],
   "source": [
    "#Seting up the Model\n",
    "criticality_network.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQwpKXySMGBH"
   },
   "source": [
    "### Training the criticality network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-2N8Q80L-Yg"
   },
   "outputs": [],
   "source": [
    "#Data set organization\n",
    "from tempfile import mkdtemp\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memoization \n",
    "file_corpora_train_x = path.join(mkdtemp(), 'train_x.dat') #Update per experiment\n",
    "file_corpora_test_x = path.join(mkdtemp(), 'test_x.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memoization \n",
    "file_corpora_train_x = path.join(mkdtemp(), 'train_x.dat') #Update per experiment\n",
    "file_corpora_test_x = path.join(mkdtemp(), 'test_x.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLzqqGd7MUwA"
   },
   "outputs": [],
   "source": [
    "#Shaping\n",
    "shape_train_x = (len(corpora_train),max_len_sentences,embeddigs_cols,1)\n",
    "shape_test_x = (len(corpora_test),max_len_sentences,embeddigs_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWvQjALqMZL4"
   },
   "outputs": [],
   "source": [
    "#Data sets\n",
    "corpora_train_x = np.memmap(\n",
    "        filename = file_corpora_train_x, \n",
    "        dtype='float32', \n",
    "        mode='w+', \n",
    "        shape = shape_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivCP2HQlMdFQ"
   },
   "outputs": [],
   "source": [
    "corpora_test_x = np.memmap( #Test Corpora (for future evaluation)\n",
    "        filename = file_corpora_test_x, \n",
    "        dtype='float32', \n",
    "        mode='w+', \n",
    "        shape = shape_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZCT_mVRMflg"
   },
   "outputs": [],
   "source": [
    "target_train_y = np.array(target_train) #Train Target\n",
    "target_test_y = np.array(target_test) #Test Target (for future evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gtb-8-VMoYL"
   },
   "outputs": [],
   "source": [
    "#Reshaping Train Inputs\n",
    "for doc in range(len(corpora_train)):\n",
    "    #print(corpora_train[doc].shape[1])\n",
    "    for words_rows in range(corpora_train[doc].shape[0]):\n",
    "        embed_flatten = np.array(corpora_train[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "        for embedding_cols in range(embed_flatten.shape[0]):\n",
    "            corpora_train_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9hH0IbAMo5L"
   },
   "outputs": [],
   "source": [
    "#Reshaping Test Inputs (for future evaluation)\n",
    "for doc in range(len(corpora_test)):\n",
    "    for words_rows in range(corpora_test[doc].shape[0]):\n",
    "        embed_flatten = np.array(corpora_test[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "        for embedding_cols in range(embed_flatten.shape[0]):\n",
    "            corpora_test_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgHs7WwRNCNe"
   },
   "outputs": [],
   "source": [
    "#CheckPoints\n",
    "filepath = \"best_model.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqB2PbvYNHFV"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks_list = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(corpora_train_x)\n",
    "mask = np.random.choice(length, length, replace=False)\n",
    "# Use the same mask to maintain the shuffling sequence between data and labels\n",
    "shuffled_data = corpora_train_x[mask]\n",
    "shuffled_labels = target_train_y[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "30ahQNHeNIxg",
    "outputId": "d55fd7e2-a3b7-4adf-e744-2a1545d87af2"
   },
   "outputs": [],
   "source": [
    "#Model Fitting\n",
    "history = criticality_network.fit(\n",
    "            x = shuffled_data, \n",
    "            y = shuffled_labels,\n",
    "            batch_size=64,\n",
    "            epochs=40, #5 <------ Hyperparameter\n",
    "            validation_split = 0.2,\n",
    "            callbacks=callbacks_list,\n",
    "            shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = 0\n",
    "count_2 = 0\n",
    "for i in target_train_y:\n",
    "    if i[0] == 1:\n",
    "        count_1 +=1 \n",
    "    else:\n",
    "        count_2 +=1\n",
    "print('{}, {}'.format(count_1, count_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xz8wz1ZPNKmP"
   },
   "outputs": [],
   "source": [
    "#Saving Training History\n",
    "df_history = pd.DataFrame.from_dict(history.history)\n",
    "df_history.to_csv('history_training.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "lmHTkzrZNQB7",
    "outputId": "9f7bdc82-5c94-47a3-9b9b-4b83ff83cb86"
   },
   "outputs": [],
   "source": [
    "criticality_network.save(filepath)\n",
    "df_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2FraZbmNWjH"
   },
   "outputs": [],
   "source": [
    "#Saving Test Data\n",
    "np.save('corpora_test_x.npy',corpora_test_x)\n",
    "np.save('target_test_y.npy',target_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_ZppHO9NcsA"
   },
   "source": [
    "### Partial Evaluation\n",
    "To make a deep evaluation, please refer to the \"evaluation\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "ngK3GrrRNp_h",
    "outputId": "223f1241-bb79-4130-99a4-ee02acf11bbf"
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs2 = range(len(acc))\n",
    " \n",
    "plt.plot(epochs2, acc, 'b', label='Training')\n",
    "plt.plot(epochs2, val_acc, 'r', label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs2, loss, 'b', label='Training')\n",
    "plt.plot(epochs2, val_loss, 'r', label='Validation')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "alpha-securereqnet.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
